{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import time\n",
    "import json\n",
    "from datetime import timedelta, date\n",
    "import math\n",
    "\n",
    "# Here we store our API read key in a string variable that we can reference later.\n",
    "my_api_read_key = os.environ['PURPLEAIR_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful links about the PurpleAir API and about air quality measureing:\n",
    "- [What is the Difference Between CF=1, ATM, and ALT?](https://community.purpleair.com/t/what-is-the-difference-between-cf-1-atm-and-alt/6442)\n",
    "- [Loop API Calls for Historical Data](https://community.purpleair.com/t/loop-api-calls-for-historical-data/4623)\n",
    "- [API field descriptions](https://community.purpleair.com/t/api-fields-descriptions/4652)\n",
    "- [How to Make Efficient API Calls](https://community.purpleair.com/t/how-to-make-efficient-api-calls/6906)\n",
    "- [Python script for downloading and organizing historical API data](https://community.purpleair.com/t/python-script-for-downloading-and-organizing-historical-api-data/3726)\n",
    "- [Calibration of PurpleAir monitors](https://community.purpleair.com/t/calibration-of-purpleair-monitors/482/3)\n",
    "- [Python script: Get Historical Data](https://github.com/alamp326/PA_DataScripts/blob/main/pa_get_historicaldata_bygroup.py)\n",
    "- I'm starting to wonder if some of the advice in these community posts are outdated... seems like maybe the ALT field is what does the calibration because the [Sensors - Get Sensor Data > Sensor data fields >  pm2.5_alt](https://api.purpleair.com/) seems to suggest that the calculation is baked in... but I think Lance recommends 3.4 now not just 3 which is maybe the root of [his May 1 comment here](https://community.purpleair.com/t/what-is-the-difference-between-cf-1-atm-and-alt/6442/14?u=akanik)... I'm going to need to create a new community post or email someone.\n",
    "\n",
    "Also, if you have any questions about PM2.5 and the science behind it... [Lance Arthur Wallace](https://community.purpleair.com/u/lance/summary) is probably your guy. He has helped PurpleAir reverse engineer some really nerding sounding shit having to do with how the monitors detect different types of PM2.5.\n",
    "\n",
    "It seems like the PM2.5 ALT is what I want to pull, but according to the comments on [this article](https://community.purpleair.com/t/what-is-the-difference-between-cf-1-atm-and-alt/6442/14), it also seems like I'll need to do some maths to correct the values produced here. Namely, I need to multiply the values by 3.4/3 (1.333333) to get the most accurate reading.\n",
    "\n",
    "**Update to methodology 2024.12.31:**\n",
    "\n",
    "I will message Lance/PurpleAir to make sure i have this right but... I think this is the way forward after hours of reading the docs, specifically the current API docs for PM2.5 ALT found here [Sensors - Get Sensor Data > Sensor data fields >  pm2.5_alt](https://api.purpleair.com/) and this May 1, 2024 comment from Lance found [here](https://community.purpleair.com/t/what-is-the-difference-between-cf-1-atm-and-alt/6442/14?u=akanik): \n",
    "- use PM2.5 ALT\n",
    "- multiply the PM2.5 ALT value by 3.4/3.0 to implement the most current correction value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all houmetro regardless of recency of data or sensor age: '../GIS/purpleair/houmetro-pa-sensors.csv'\n",
    "#houmetro with data from last 30 days and sensors older than 1 year: '../GIS/purpleair/houmetro-pa-sensors-atleast_1yr.csv'\n",
    "houmetro_sensors = pd.read_csv('../GIS/purpleair/houmetro-pa-sensors-atleast_1yr.csv')\n",
    "\n",
    "#if you want to only grab data for outdoor sensors, uncomment the following line\n",
    "#houmetro_sensors = houmetro_sensors.loc[houmetro_sensors['location_type'] == 0]\n",
    "\n",
    "#for testing purposes we're gonna limit to like 5 of these\n",
    "# houmetro_sensors = houmetro_sensors.sample(2)\n",
    "# display(houmetro_sensors)\n",
    "print(len(houmetro_sensors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHistSensorData(sensor_index,my_fields,other_params):\n",
    "    \n",
    "    api_url = f'https://api.purpleair.com/v1/sensors/{sensor_index}/history'\n",
    "\n",
    "    # my_headers is assigned the context of our request we want to make. In this case\n",
    "    # we will pass through our API read key using the variable created above.\n",
    "    my_headers = {'X-API-Key':my_api_read_key}\n",
    "\n",
    "    # my_params is assigned a list of fields of data we are requesting. Excluding the\n",
    "    # fields parameter will collect all available fields.\n",
    "    # example input structure: 'temperature,pm2.5_atm'\n",
    "    field_param = {'fields':my_fields}\n",
    "    \n",
    "    #other_params should be a dict, see the following for options:\n",
    "    # https://api.purpleair.com/#api-sensors-get-sensor-history\n",
    "    my_params = {**field_param,**other_params}\n",
    "\n",
    "    # This line creates and sends the request and then assigns its response to the\n",
    "    # variable, r.\n",
    "    r = requests.get(api_url, headers=my_headers, params=my_params)\n",
    "\n",
    "    # We then return the response we received.\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#SETTING UP SOME VARIABLES\n",
    "#############################################\n",
    "\n",
    "fields = ['pm2.5_alt_a','pm2.5_alt_b']\n",
    "fields_str = ','.join(fields)\n",
    "\n",
    "#the average time period for the data\n",
    "#if you change this, all of the shit below needs to change too cause you were too\n",
    "#lazy to turn this into a function. asshole.\n",
    "data_avg = 1440\n",
    "\n",
    "#be kind, don't overload the server\n",
    "sleep_sec = 1\n",
    "\n",
    "#establish start/end dates\n",
    "start_timestamp = '2022-01-01T00:00:00'\n",
    "end_timestamp = '2024-12-31T00:00:00'\n",
    "\n",
    "#i'm also going to record which sensor and timeframe we got data for before\n",
    "#inevitable issues popup and i need to fucking start over and waste a bunch of\n",
    "#PA credits\n",
    "successful_pulls = {}\n",
    "\n",
    "\n",
    "#############################################\n",
    "#WORKING WITH TIME\n",
    "#############################################\n",
    "#there are limits on how much data can be pulled based on the data_avg parameter. \n",
    "#for 1440 (day) the limit is 2 years worth of data. We'll need to adjust our \n",
    "#request to pull data in chunks if we want more than that.\n",
    "\n",
    "#i'm not going to waste time and create a condition for all of these right now, \n",
    "#but once i have to do this enough times, I can create a function to handle this\n",
    "#Time period    |   data_avg value  |   historical limit\n",
    "#_______________________________________________________\n",
    "#Real-time      |   0               |   30 days\n",
    "#10-minute\t    |   10              |   60 days\n",
    "#30-minute\t    |   30              |   90 days\n",
    "#1-hour\t        |   60              |   180 days\n",
    "#6-hour\t        |   360             |   1 year\n",
    "#1-day          |   1440            |   2 years\n",
    "#1-week\t        |   10080           |   5 years\n",
    "#1-month        |   43200           |   20 years\n",
    "#1-year\t        |   525600          |   100 years\n",
    "\n",
    "#get timestamp from date\n",
    "start_timestamp = pd.to_datetime(start_timestamp)\n",
    "end_timestamp = pd.to_datetime(end_timestamp)\n",
    "\n",
    "timeframe_pulls = [[start_timestamp,end_timestamp]]\n",
    "\n",
    "historical_limit = timedelta(days=365*2).total_seconds()/3600\n",
    "curr_timeframe = (timeframe_pulls[0][1] - timeframe_pulls[0][0]).total_seconds()/3600\n",
    "# print('historical_limit',historical_limit)\n",
    "# print('curr_timeframe',curr_timeframe)\n",
    "\n",
    "new_timeframe_pulls = []\n",
    "\n",
    "if data_avg == 1440 and curr_timeframe > historical_limit:\n",
    "    print('Data range exceeds 2 years, engaging in multiple pulls')\n",
    "    \n",
    "    #how many pulls will we need to do? we're rounding up cause we want to make sure\n",
    "    #we get all the data\n",
    "    pulls = math.ceil(curr_timeframe / historical_limit)\n",
    "    print('Pulls necessary:',pulls)\n",
    "    \n",
    "    og_start = timeframe_pulls[0][0]\n",
    "    for i in range(0,int(pulls)):\n",
    "        if i == 0:\n",
    "            new_start = og_start\n",
    "        else:\n",
    "            new_start = new_timeframe_pulls[i-1][1]\n",
    "        new_end = new_start + timedelta(days=365*2)\n",
    "        if new_end > pd.Timestamp(date.today()):\n",
    "            new_end = pd.Timestamp(date.today() - timedelta(days=1))\n",
    "        new_timeframe_pulls.append([new_start,new_end])\n",
    "    \n",
    "    print('New timeframe pulls:',new_timeframe_pulls)\n",
    "    \n",
    "if len(new_timeframe_pulls) == 0:\n",
    "    new_timeframe_pulls = timeframe_pulls\n",
    " \n",
    "   \n",
    "#############################################\n",
    "#DO THE LOOPDY LOOP\n",
    "#############################################\n",
    "\n",
    "dfs = []\n",
    "for timeframe in new_timeframe_pulls:\n",
    "    print('Pulling data for timeframe:',timeframe)\n",
    "    \n",
    "    timeframe_str = str(int(timeframe[0].timestamp()))+'_'+str(int(timeframe[1].timestamp()))\n",
    "    \n",
    "    successful_pulls[timeframe_str] = []\n",
    "    \n",
    "    other_params = {'average':int(data_avg),\n",
    "                    'start_timestamp':int(timeframe[0].timestamp()),\n",
    "                    'end_timestamp':int(timeframe[1].timestamp())}\n",
    "    \n",
    "    for index, row in houmetro_sensors.iterrows():\n",
    "    #for sensor_index in houmetro_sensors['sensor_index']:\n",
    "        sensor_index = row['sensor_index']\n",
    "        sensor_created = pd.to_datetime(row['date'])\n",
    "        \n",
    "        #if the sensor was created after the end of the timeframe, we don't want to pull data\n",
    "        if sensor_created > pd.to_datetime(int(timeframe[1].timestamp()),unit='s'):\n",
    "            print('Skipping sensor',sensor_index,'because it was created after the end of the timeframe')\n",
    "            continue\n",
    "        \n",
    "        sensor_filename = f'../data/analyzed/purpleair/sensor-data/pa-sensor-{sensor_index}-hist-2022-2024.csv'\n",
    "        sensor_data = getHistSensorData(int(sensor_index),fields_str,other_params)\n",
    "        sensor_data_json = sensor_data.json()\n",
    "        print(sensor_data_json)\n",
    "        if 'data' in sensor_data_json and len(sensor_data_json['data']) > 0:\n",
    "            if len(sensor_data_json['data']) > 0:\n",
    "                hist_df = pd.DataFrame(sensor_data_json['data'],columns=sensor_data_json['fields'])\n",
    "                hist_df['sensor_index'] = sensor_index\n",
    "                \n",
    "                successful_pulls[timeframe_str].append(sensor_index)\n",
    "                \n",
    "                if os.path.exists(sensor_filename):\n",
    "                    prev_data = pd.read_csv(sensor_filename)\n",
    "                    hist_df = pd.concat([prev_data,hist_df])\n",
    "                \n",
    "                hist_df.to_csv(sensor_filename,index=False)\n",
    "                \n",
    "                dfs.append(hist_df)\n",
    "        time.sleep(sleep_sec)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat and save real quick. Then we can process in the next script\n",
    "houmetro_sensor_data = pd.concat(dfs)\n",
    "houmetro_sensor_data.to_csv('../data/analyzed/purpleair/houmetro-pa-2022-2024-pm25.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a little testy-poo to make sure things are lining up the way we thought they would\n",
    "sensor_index = str(161015)\n",
    "test_file = f'../data/analyzed/purpleair/sensor-data/pa-sensor-{sensor_index}-hist-2022-2024.csv'\n",
    "\n",
    "test_df = pd.read_csv(test_file)\n",
    "test_df['date'] = pd.to_datetime(test_df['time_stamp'],unit='s')\n",
    "\n",
    "print(len(test_df))\n",
    "print(test_df['date'].min())\n",
    "print(test_df['date'].max())\n",
    "display(test_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
